{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0a4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Task 1\n",
    "\n",
    "corpus = sc.textFile(\"s3://chrisjermainebucket/comp330_A5/TrainingDataOneLinePerDoc.txt\")\n",
    "validLines = corpus.filter(lambda x: 'id' in x)\n",
    "\n",
    "keyAndText = validLines.map(lambda x: (x[x.index('id=\"') + 4: x.index('\" url=')], x[x.index('\">') + 2:]))\n",
    "\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "keyAndListOfWords = keyAndText.map(lambda x: (str(x[0]), regex.sub(' ', x[1]).lower().split()))\n",
    "\n",
    "allWords = keyAndListOfWords.flatMap(lambda x: ((j, 1) for j in x[1]))\n",
    "allCounts = allWords.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "topWords = allCounts.top(20000, lambda x: x[1])\n",
    "twentyK = sc.parallelize(range(20000))\n",
    "\n",
    "dictionary = twentyK.map(lambda x: (topWords[x][0], x))\n",
    "\n",
    "mapDict = dictionary.collectAsMap()\n",
    "\n",
    "print('applicant:', mapDict['applicant'])\n",
    "print('and:', mapDict['and'])\n",
    "print('attack:', mapDict['attack'])\n",
    "print('protein:', mapDict['protein'])\n",
    "print('car:', mapDict['car'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e4d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "numDocs = keyAndListOfWords.keys().count()\n",
    "\n",
    "def docWordCounts(doc):\n",
    "    id, wordList = doc\n",
    "    wordCounts = np.zeros(20000)\n",
    "    for word in wordList:\n",
    "        if word in mapDict:\n",
    "            order = mapDict[word]\n",
    "            wordCounts[order] += 1\n",
    "    return (id, wordCounts)\n",
    "\n",
    "docWords = keyAndListOfWords.map(docWordCounts)\n",
    "\n",
    "def binaryFreq(doc):\n",
    "    id, wordCounts = doc\n",
    "    binary = np.array([1 if i > 0 else 0 for i in wordCounts])\n",
    "    return (id, binary)\n",
    "\n",
    "binaries = docWords.map(binaryFreq)\n",
    "freq = binaries.map(lambda x: x[1]).reduce(lambda a, b: a + b)\n",
    "idf = np.log(numDocs / freq)\n",
    "\n",
    "def tfIDF(doc):\n",
    "    id, wordCounts = doc\n",
    "    tf = wordCounts / np.sum(wordCounts)\n",
    "    tf_idf = np.array(tf * idf)\n",
    "    return (id, tf_idf)\n",
    "\n",
    "tfVectors = docWords.map(tfIDF)\n",
    "tfVectors.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c86945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularize the TF IDF Vectors\n",
    "\n",
    "# Get mean vector\n",
    "tfs = tfVectors.map(lambda x: x[1])\n",
    "summed_tf = tfs.reduce(lambda a,b: a + b)\n",
    "mean_tf = np.array([val/numDocs for val in summed_tf])\n",
    "\n",
    "# Get std vector\n",
    "# Use square differences to solve for std vector\n",
    "def squareDiff(vec):\n",
    "    arr = np.array(vec)\n",
    "    return np.square(arr)\n",
    "\n",
    "diff_tf = tfs.map(lambda x: x - mean_tf)\n",
    "\n",
    "squared_tf = diff_tf.map(lambda x: squareDiff(x))\n",
    "\n",
    "total_tf = squared_tf.reduce(lambda a, b: a + b)\n",
    "\n",
    "std_tf = np.sqrt(total_tf / numDocs)\n",
    "\n",
    "def regularize(doc):\n",
    "    docID, tfVect = doc\n",
    "    temp1 = tfVect - mean_tf\n",
    "        \n",
    "    reg = temp1 / std_tf\n",
    "    \n",
    "    reg[std_tf == 0.0] = 0.0\n",
    "    \n",
    "    label = 0\n",
    "    \n",
    "    if docID[:2] == 'AU':\n",
    "        label = 1\n",
    "    \n",
    "    return (label, reg)\n",
    "\n",
    "regularVect = tfVectors.map(lambda x: regularize(x))\n",
    "\n",
    "regularVect.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4dead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3) Gradient Descent\n",
    "\n",
    "# divide by numDocs for NaN issues\n",
    "r = np.zeros(20000)\n",
    "z = 0.0001\n",
    "\n",
    "def llh(doc):\n",
    "    y, x = doc\n",
    "    theta = np.dot(x, r)\n",
    "    yTheta = y * theta\n",
    "    logTerm = np.log(1 + np.exp(theta))\n",
    "    return np.sum(yTheta - logTerm) / numDocs\n",
    " \n",
    "def gradientF(doc):\n",
    "    y, x = doc\n",
    "    gradient = np.zeros(20000)\n",
    "    theta = np.dot(x, r)\n",
    "    last = 2 * z * r\n",
    "    sig = np.exp(theta) / (1 + np.exp(theta))\n",
    "    total = -1 * x * y + np.dot(x.T, sig)\n",
    "    gradient += total\n",
    "    gradient += (2 * z * r)\n",
    "    return gradient / numDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f48c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample gradient \n",
    "sampleVect = regularVect.sample(False, 0.1)\n",
    "\n",
    "rate = 0.1\n",
    "\n",
    "currSampleLoss = sampleVect.map(lambda x: -1 * llh(x)).reduce(lambda a,b: a + b)\n",
    "prevSampleLoss = 1000\n",
    "\n",
    "while abs(currSampleLoss - prevSampleLoss) > 10e-4:\n",
    "    \n",
    "    prevSampleLoss = currSampleLoss\n",
    "    sample_gradient = sampleVect.map(lambda x: gradientF(x)).reduce(lambda a,b: a + b)\n",
    "    r = r - rate * sample_gradient\n",
    "    \n",
    "    regSampleTerm = z * np.sum(np.square(r))\n",
    "    currSampleLoss = sampleVect.map(lambda x: -1 * llh(x)).reduce(lambda a,b: a + b) + regSampleTerm\n",
    "    \n",
    "    if currSampleLoss > prevSampleLoss: \n",
    "        rate = rate * 0.5\n",
    "    else:\n",
    "        rate = rate * 1.1\n",
    "    \n",
    "    print(currSampleLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af259725",
   "metadata": {},
   "outputs": [],
   "source": [
    "currLoss = regularVect.map(lambda x: -1 * llh(x)).reduce(lambda a,b: a + b)\n",
    "prevLoss = 1\n",
    "\n",
    "while abs(currLoss - prevLoss) > 10e-4:\n",
    "    \n",
    "    prevLoss = currLoss\n",
    "    gradient = regularVect.map(lambda x: gradientF(x)).reduce(lambda a,b: a + b)\n",
    "    r = r - rate * gradient\n",
    "    \n",
    "    regTerm = z * np.sum(np.square(r))\n",
    "    currLoss = regularVect.map(lambda x: -1 * llh(x)).reduce(lambda a,b: a + b) + regTerm\n",
    "    \n",
    "    if currLoss > prevLoss: \n",
    "        rate = rate * 0.5\n",
    "    else:\n",
    "        rate = rate * 1.1\n",
    "    \n",
    "    print(currLoss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a695b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(r)[-50:]\n",
    "words = [key for key, value in mapDict.items() if value in indices]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e849b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testingData = sc.textFile(\"s3://chrisjermainebucket/comp330_A5/TestingDataOneLinePerDoc.txt\")\n",
    "testLines = testingData.filter(lambda x: 'id' in x)\n",
    "testkeyAndText = testLines.map(lambda x: (x[x.index('id=\"') + 4: x.index('\" url=')], x[x.index('\">') + 2:]))\n",
    "\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "testWords = testkeyAndText.map(lambda x: (str(x[0]), regex.sub(' ', x[1]).lower().split()))\n",
    "\n",
    "testDoc = testWords.map(docWordCounts)\n",
    "testBin = testDoc.map(binaryFreq)\n",
    "testFreq = testBin.map(lambda x: x[1]).reduce(lambda a, b: a + b)\n",
    "\n",
    "testTFIDF = testDoc.map(tfIDF)\n",
    "\n",
    "def testRegularize(doc):\n",
    "    docID, tfVect = doc\n",
    "    temp1 = tfVect - mean_tf\n",
    "        \n",
    "    reg = temp1 / (10 * std_tf)\n",
    "    \n",
    "    reg[std_tf == 0.0] = 0.0\n",
    "    \n",
    "    label = 0\n",
    "    \n",
    "    if docID[:2] == 'AU':\n",
    "        label = 1\n",
    "    \n",
    "    return (docID, label, reg)\n",
    "\n",
    "testReg = testTFIDF.map(lambda x: testRegularize(x))\n",
    "\n",
    "\n",
    "def predictLabel(doc):\n",
    "    \n",
    "    docID, y, x = doc\n",
    "    \n",
    "    numActuallyTrue = 0 # denominator for recall\n",
    "    numWeSayTrue = 0 # denominator for precision\n",
    "    \n",
    "    success = 0 # numerator for precision and recall\n",
    "    \n",
    "    sucNeg = 0\n",
    "    \n",
    "    falsePos = 0 # need to find documents that were false positives\n",
    "    \n",
    "    falseNeg = 0 # not really needed\n",
    "    \n",
    "    cutoff = 0.6\n",
    "    \n",
    "    if np.dot(x, r) > cutoff:\n",
    "        numWeSayTrue = 1\n",
    "    \n",
    "    if y > cutoff:\n",
    "        numActuallyTrue = 1\n",
    "    \n",
    "    if (np.dot(x, r) > cutoff) and (y > cutoff):\n",
    "        success = 1\n",
    "        \n",
    "    elif (np.dot(x, r) < cutoff) and (y < cutoff):\n",
    "        sucNeg = 1\n",
    "\n",
    "    elif (np.dot(x, r) > cutoff) and (y < cutoff):\n",
    "        falsePos = 1\n",
    "        \n",
    "    else:\n",
    "        falseNeg = 1\n",
    "    \n",
    "    return (docID, success, numActuallyTrue, numWeSayTrue, falsePos, np.dot(x,r), y)\n",
    "\n",
    "testResults = testReg.map(lambda x: predictLabel(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e7b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find F1\n",
    "\n",
    "numSuccesses = testResults.map(lambda x: x[1]).reduce(lambda a,b: a + b)\n",
    "numActuallyTrues = testResults.map(lambda x: x[2]).reduce(lambda a,b: a + b)\n",
    "numWeSayTrues = testResults.map(lambda x: x[3]).reduce(lambda a,b: a + b)\n",
    "\n",
    "print(numWeSayTrues, numActuallyTrues, numSuccesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6201ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = numSuccesses / numWeSayTrues\n",
    "recall = numSuccesses / numActuallyTrues\n",
    "\n",
    "F1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"F1:\",F1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c493e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "numFP = testResults.map(lambda x: x[4]).reduce(lambda a,b: a + b)\n",
    "print(numFP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b4d1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFP(doc):\n",
    "    docID, success, numActuallyTrue, numWeSayTrue, falsePos, val, y = doc\n",
    "    return \n",
    "    \n",
    "fpDocs = testResults.filter(lambda x: x[4] > 0).map(lambda x: x[0]).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb17a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fpDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42789f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testWords.lookup('35797415'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa7cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testWords.lookup('3470592'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba03bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testWords.lookup('19505797'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
